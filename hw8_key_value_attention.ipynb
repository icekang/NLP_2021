{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "hw8_key_value_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icekang/NLP_2021/blob/main/hw8_key_value_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1cDcKRZwXCL"
      },
      "source": [
        "# Key-Value Attention Mechanism Homework on Keras: Character-level Machine Translation (Many-to-Many, encoder-decoder)\n",
        "\n",
        "In this homework, you will create an MT model with key-value attention mechnism that coverts names of constituency MP candidates in the 2019 Thai general election from Thai script to Roman(Latin) script. E.g. นิยม-->niyom "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy6QYsP4wa-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2612901d-101f-4055-e7c7-178b1032d1f0"
      },
      "source": [
        "!wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.font_manager.fontManager.addfont('thsarabunnew-webfont.ttf') # 3.2+\n",
        "mpl.rc('font', family='TH Sarabun New')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-21 15:08:50--  https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Phonbopit/sarabun-webfont/master/fonts/thsarabunnew-webfont.ttf [following]\n",
            "--2021-03-21 15:08:51--  https://raw.githubusercontent.com/Phonbopit/sarabun-webfont/master/fonts/thsarabunnew-webfont.ttf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 98308 (96K) [application/octet-stream]\n",
            "Saving to: ‘thsarabunnew-webfont.ttf’\n",
            "\n",
            "thsarabunnew-webfon 100%[===================>]  96.00K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-03-21 15:08:51 (18.7 MB/s) - ‘thsarabunnew-webfont.ttf’ saved [98308/98308]\n",
            "\n",
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRdbTrQJwXCR"
      },
      "source": [
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq20keO6wXCh"
      },
      "source": [
        "## Load Dataset\n",
        "We have generated a toy dataset using names of constituency MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/images/dataset_diagram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8lWBh40wjgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40186f63-fe21-435a-cc67-d0e3845c31e0"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-21 15:08:52--  https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 324399 (317K) [text/plain]\n",
            "Saving to: ‘mp_name_th_en.csv’\n",
            "\n",
            "mp_name_th_en.csv   100%[===================>] 316.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-03-21 15:08:52 (19.2 MB/s) - ‘mp_name_th_en.csv’ saved [324399/324399]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTQk8W4OwXCk"
      },
      "source": [
        "import csv\n",
        "with open('mp_name_th_en.csv') as csvfile:\n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    name_th = []\n",
        "    name_en = []\n",
        "    for row in readCSV:\n",
        "        name_th.append(row[0])\n",
        "        name_en.append(row[1])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVNHVM_FwXCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d3c243e-8b13-4015-b134-9e7b877e48a5"
      },
      "source": [
        "for th, en in zip(name_th[:10],name_en[:10]):\n",
        "    print(th,en)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ไกรสีห์ kraisi\n",
            "พัชรี phatri\n",
            "ธีระ thira\n",
            "วุฒิกร wutthikon\n",
            "ไสว sawai\n",
            "สัมภาษณ์  samphat\n",
            "วศิน wasin\n",
            "ทินวัฒน์ thinwat\n",
            "ศักดินัย sakdinai\n",
            "สุรศักดิ์ surasak\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heMTiM7qwXC2"
      },
      "source": [
        "## Task1: Preprocess dataset for Keras (1 point)\n",
        "* 2 dictionaries for indexing (1 for input and another for output)\n",
        "* DON'T FORGET TO INCLUDE special token for padding\n",
        "* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)\n",
        "* Be mindful of your pad_sequences \"padding\" hyperparameter. Choose wisely (post-padding vs pre-padding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O5YhjntwXC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab312e08-9dfc-4a6e-a4d6-a34ae8e35711"
      },
      "source": [
        "#FILL YOUR CODE HERE\n",
        "\n",
        "#Preprocessing\n",
        "input_chars = list(set(''.join(name_th)))\n",
        "output_chars = list(set(''.join(name_en)))\n",
        "\n",
        "# +1 for padding\n",
        "data_size, vocab_size = len(name_th), len(input_chars)+1 \n",
        "output_vocab_size = len(output_chars)+1\n",
        "\n",
        "print('There are %d lines and %d unique characters in your input data.' % (data_size, vocab_size))\n",
        "maxlen = len(max(name_th, key=len)) #max input length\n",
        "\n",
        "output_maxlen = len(max(name_en, key=len)) #max input length\n",
        "\n",
        "print(\"Max input length:\", maxlen)\n",
        "\n",
        "sorted_chars= sorted(input_chars)\n",
        "sorted_output_chars= sorted(output_chars)\n",
        "sorted_chars.insert(0,\"<PAD>\") #PADDING for input\n",
        "sorted_output_chars.insert(0,\"<PAD>\") #PADDING for output\n",
        "#Input\n",
        "char_to_ix = { ch:i for i,ch in enumerate(sorted_chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(sorted_chars) } #reverse dictionary\n",
        "#Output\n",
        "output_char_to_ix = { ch:i for i,ch in enumerate(sorted_output_chars) }\n",
        "ix_to_output_char = { i:ch for i,ch in enumerate(sorted_output_chars) } #reverse dictionary\n",
        "\n",
        "print(ix_to_char)\n",
        "\n",
        "m=data_size\n",
        "Tx=maxlen\n",
        "Ty=output_maxlen\n",
        "\n",
        "X = []\n",
        "for line in name_th:\n",
        "    temp=[]\n",
        "    for char in line:\n",
        "        temp.append(char_to_ix[char])\n",
        "    X.append(temp)\n",
        "Y = []\n",
        "for line in name_en:\n",
        "    temp=[]\n",
        "    for char in line:\n",
        "        temp.append(output_char_to_ix[char])\n",
        "    Y.append(temp)    \n",
        "\n",
        "X = pad_sequences(X,maxlen=maxlen)\n",
        "Y = pad_sequences(Y,maxlen=output_maxlen)\n",
        "\n",
        "X= to_categorical(X,vocab_size)\n",
        "X=X.reshape(data_size,maxlen ,vocab_size)\n",
        "\n",
        "Y= to_categorical(Y,output_vocab_size)\n",
        "Y=Y.reshape(data_size,output_maxlen,output_vocab_size)\n",
        "print(X.shape,Y.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 10887 lines and 65 unique characters in your input data.\n",
            "Max input length: 20\n",
            "{0: '<PAD>', 1: ' ', 2: 'ก', 3: 'ข', 4: 'ค', 5: 'ฆ', 6: 'ง', 7: 'จ', 8: 'ฉ', 9: 'ช', 10: 'ซ', 11: 'ฌ', 12: 'ญ', 13: 'ฎ', 14: 'ฏ', 15: 'ฐ', 16: 'ฑ', 17: 'ฒ', 18: 'ณ', 19: 'ด', 20: 'ต', 21: 'ถ', 22: 'ท', 23: 'ธ', 24: 'น', 25: 'บ', 26: 'ป', 27: 'ผ', 28: 'ฝ', 29: 'พ', 30: 'ฟ', 31: 'ภ', 32: 'ม', 33: 'ย', 34: 'ร', 35: 'ล', 36: 'ว', 37: 'ศ', 38: 'ษ', 39: 'ส', 40: 'ห', 41: 'ฬ', 42: 'อ', 43: 'ฮ', 44: 'ะ', 45: 'ั', 46: 'า', 47: 'ำ', 48: 'ิ', 49: 'ี', 50: 'ึ', 51: 'ื', 52: 'ุ', 53: 'ู', 54: 'เ', 55: 'แ', 56: 'โ', 57: 'ใ', 58: 'ไ', 59: '็', 60: '่', 61: '้', 62: '๊', 63: '๋', 64: '์'}\n",
            "(10887, 20, 65) (10887, 19, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNqqnkVSwXC-"
      },
      "source": [
        "# Attention Mechanism\n",
        "## Task 2: Code your own (key-value) attention mechnism (1 point)\n",
        "* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n",
        "* Define global variables\n",
        "* fill code for one_step_attention function\n",
        "* Hint: use keras.layers.Lambda \n",
        "* HINT: you will probably need more hidden dimmensions than what you've seen in the demo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSdFcuGuwXDB"
      },
      "source": [
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.layers import Lambda\n",
        "def softMaxAxis1(x):\n",
        "    return softmax(x,axis=1)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS3Ziti1wXDH"
      },
      "source": [
        "#These are global variables (shared layers)\n",
        "## Fill your code here\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "#Attention function###\n",
        "fattn_1 = Dense(10, activation = \"tanh\", name='fattn_1')\n",
        "fattn_2 = Dense(1, activation = \"relu\", name='fattn_2')\n",
        "###\n",
        "activator = Activation(softMaxAxis1, name='attention_scores') \n",
        "dotor = Dot(axes = 1)\n",
        "## you are allowed to use code in the demo as your template.  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbIj5oo_RymZ"
      },
      "source": [
        "n_h = 128 #hidden dimensions for encoder \n",
        "n_s = 256 #hidden dimensions for decoder"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecNci8x5wXDN"
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "\n",
        "    #Fill code here\n",
        "    a_key = Lambda(lambda x: x[:,:,:n_h])(a)\n",
        "    a_val = Lambda(lambda x: x[:,:,n_h:])(a)\n",
        "    # Repeat the decoder hidden state to concat with encoder hidden states\n",
        "    s_prev = repeator(s_prev)\n",
        "    concat = concatenator([a_key,s_prev])\n",
        "    # attention function\n",
        "    e = fattn_1(concat)\n",
        "    energies =fattn_2(e) # (None, 1)\n",
        "    # calculate attention_scores (softmax)\n",
        "    attention_scores = activator(energies) #(None, 1)\n",
        "    #calculate a context vector\n",
        "    context = dotor([attention_scores,a_val])\n",
        "\n",
        "    return context # return whatever you need to complete this homework "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bgSCY3NwXDU"
      },
      "source": [
        "## Task3: Create and train your encoder/decoder model here (1 point)\n",
        "* HINT: you will probably need more hidden dimmensions than what you've seen in the demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CVrgh9nwXDV"
      },
      "source": [
        "#FILL CODE HERE\n",
        "encoder_LSTM =  Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(-1, Tx, n_h*2), name='encoder')\n",
        "decoder_LSTM_cell = LSTM(n_s, return_state = True, name='decoder') #decoder_LSTM_cell\n",
        "output_layer = Dense(output_vocab_size, activation=\"softmax\") #softmax output layer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XptuOQj-wXDb"
      },
      "source": [
        "#FIT YOUR MODEL HERE\n",
        "def model(Tx, Ty, n_h, n_s, vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_h -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    vocab_size -- size of the input vocab\n",
        "    output_vocab_size -- size of the output vocab\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input of your model\n",
        "    X = Input(shape=(Tx, vocab_size), name='input_name')\n",
        "    # Define hidden state and cell state for decoder_LSTM_Cell\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = list()\n",
        "\n",
        "    #Encoder Bi-LSTM\n",
        "    # h = Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(-1, Tx, n_h*2))(X)\n",
        "    h = encoder_LSTM(X)\n",
        "    #Iterate for Ty steps (Decoding)\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
        "        context = one_step_attention(h, s)\n",
        "        # Feed the context vector to the decoder LSTM cell\n",
        "        s, _, c = decoder_LSTM_cell(context,initial_state=[s,c])\n",
        "\n",
        "        # Pass the decoder hidden output to the output layer (softmax)\n",
        "        out = output_layer(s)\n",
        "        \n",
        "        # Append an output list with the current output\n",
        "        outputs.append(out)\n",
        "    \n",
        "    #Create model instance\n",
        "    model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a8yHTcjSBgm"
      },
      "source": [
        "model = model(Tx, Ty, n_h, n_s, vocab_size, output_vocab_size)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpgmGCOpSCyh",
        "outputId": "e4a34633-9285-476d-90e3-80244fb9f08d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_name (InputLayer)         [(None, 20, 65)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Bidirectional)         (None, 20, 256)      198656      input_name[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 20, 256)      0           s0[0][0]                         \n",
            "                                                                 decoder[0][0]                    \n",
            "                                                                 decoder[1][0]                    \n",
            "                                                                 decoder[2][0]                    \n",
            "                                                                 decoder[3][0]                    \n",
            "                                                                 decoder[4][0]                    \n",
            "                                                                 decoder[5][0]                    \n",
            "                                                                 decoder[6][0]                    \n",
            "                                                                 decoder[7][0]                    \n",
            "                                                                 decoder[8][0]                    \n",
            "                                                                 decoder[9][0]                    \n",
            "                                                                 decoder[10][0]                   \n",
            "                                                                 decoder[11][0]                   \n",
            "                                                                 decoder[12][0]                   \n",
            "                                                                 decoder[13][0]                   \n",
            "                                                                 decoder[14][0]                   \n",
            "                                                                 decoder[15][0]                   \n",
            "                                                                 decoder[16][0]                   \n",
            "                                                                 decoder[17][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 20, 384)      0           lambda[0][0]                     \n",
            "                                                                 repeat_vector[0][0]              \n",
            "                                                                 lambda_2[0][0]                   \n",
            "                                                                 repeat_vector[1][0]              \n",
            "                                                                 lambda_4[0][0]                   \n",
            "                                                                 repeat_vector[2][0]              \n",
            "                                                                 lambda_6[0][0]                   \n",
            "                                                                 repeat_vector[3][0]              \n",
            "                                                                 lambda_8[0][0]                   \n",
            "                                                                 repeat_vector[4][0]              \n",
            "                                                                 lambda_10[0][0]                  \n",
            "                                                                 repeat_vector[5][0]              \n",
            "                                                                 lambda_12[0][0]                  \n",
            "                                                                 repeat_vector[6][0]              \n",
            "                                                                 lambda_14[0][0]                  \n",
            "                                                                 repeat_vector[7][0]              \n",
            "                                                                 lambda_16[0][0]                  \n",
            "                                                                 repeat_vector[8][0]              \n",
            "                                                                 lambda_18[0][0]                  \n",
            "                                                                 repeat_vector[9][0]              \n",
            "                                                                 lambda_20[0][0]                  \n",
            "                                                                 repeat_vector[10][0]             \n",
            "                                                                 lambda_22[0][0]                  \n",
            "                                                                 repeat_vector[11][0]             \n",
            "                                                                 lambda_24[0][0]                  \n",
            "                                                                 repeat_vector[12][0]             \n",
            "                                                                 lambda_26[0][0]                  \n",
            "                                                                 repeat_vector[13][0]             \n",
            "                                                                 lambda_28[0][0]                  \n",
            "                                                                 repeat_vector[14][0]             \n",
            "                                                                 lambda_30[0][0]                  \n",
            "                                                                 repeat_vector[15][0]             \n",
            "                                                                 lambda_32[0][0]                  \n",
            "                                                                 repeat_vector[16][0]             \n",
            "                                                                 lambda_34[0][0]                  \n",
            "                                                                 repeat_vector[17][0]             \n",
            "                                                                 lambda_36[0][0]                  \n",
            "                                                                 repeat_vector[18][0]             \n",
            "__________________________________________________________________________________________________\n",
            "fattn_1 (Dense)                 (None, 20, 10)       3850        concatenate[0][0]                \n",
            "                                                                 concatenate[1][0]                \n",
            "                                                                 concatenate[2][0]                \n",
            "                                                                 concatenate[3][0]                \n",
            "                                                                 concatenate[4][0]                \n",
            "                                                                 concatenate[5][0]                \n",
            "                                                                 concatenate[6][0]                \n",
            "                                                                 concatenate[7][0]                \n",
            "                                                                 concatenate[8][0]                \n",
            "                                                                 concatenate[9][0]                \n",
            "                                                                 concatenate[10][0]               \n",
            "                                                                 concatenate[11][0]               \n",
            "                                                                 concatenate[12][0]               \n",
            "                                                                 concatenate[13][0]               \n",
            "                                                                 concatenate[14][0]               \n",
            "                                                                 concatenate[15][0]               \n",
            "                                                                 concatenate[16][0]               \n",
            "                                                                 concatenate[17][0]               \n",
            "                                                                 concatenate[18][0]               \n",
            "__________________________________________________________________________________________________\n",
            "fattn_2 (Dense)                 (None, 20, 1)        11          fattn_1[0][0]                    \n",
            "                                                                 fattn_1[1][0]                    \n",
            "                                                                 fattn_1[2][0]                    \n",
            "                                                                 fattn_1[3][0]                    \n",
            "                                                                 fattn_1[4][0]                    \n",
            "                                                                 fattn_1[5][0]                    \n",
            "                                                                 fattn_1[6][0]                    \n",
            "                                                                 fattn_1[7][0]                    \n",
            "                                                                 fattn_1[8][0]                    \n",
            "                                                                 fattn_1[9][0]                    \n",
            "                                                                 fattn_1[10][0]                   \n",
            "                                                                 fattn_1[11][0]                   \n",
            "                                                                 fattn_1[12][0]                   \n",
            "                                                                 fattn_1[13][0]                   \n",
            "                                                                 fattn_1[14][0]                   \n",
            "                                                                 fattn_1[15][0]                   \n",
            "                                                                 fattn_1[16][0]                   \n",
            "                                                                 fattn_1[17][0]                   \n",
            "                                                                 fattn_1[18][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "attention_scores (Activation)   (None, 20, 1)        0           fattn_2[0][0]                    \n",
            "                                                                 fattn_2[1][0]                    \n",
            "                                                                 fattn_2[2][0]                    \n",
            "                                                                 fattn_2[3][0]                    \n",
            "                                                                 fattn_2[4][0]                    \n",
            "                                                                 fattn_2[5][0]                    \n",
            "                                                                 fattn_2[6][0]                    \n",
            "                                                                 fattn_2[7][0]                    \n",
            "                                                                 fattn_2[8][0]                    \n",
            "                                                                 fattn_2[9][0]                    \n",
            "                                                                 fattn_2[10][0]                   \n",
            "                                                                 fattn_2[11][0]                   \n",
            "                                                                 fattn_2[12][0]                   \n",
            "                                                                 fattn_2[13][0]                   \n",
            "                                                                 fattn_2[14][0]                   \n",
            "                                                                 fattn_2[15][0]                   \n",
            "                                                                 fattn_2[16][0]                   \n",
            "                                                                 fattn_2[17][0]                   \n",
            "                                                                 fattn_2[18][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1, 128)       0           attention_scores[0][0]           \n",
            "                                                                 lambda_1[0][0]                   \n",
            "                                                                 attention_scores[1][0]           \n",
            "                                                                 lambda_3[0][0]                   \n",
            "                                                                 attention_scores[2][0]           \n",
            "                                                                 lambda_5[0][0]                   \n",
            "                                                                 attention_scores[3][0]           \n",
            "                                                                 lambda_7[0][0]                   \n",
            "                                                                 attention_scores[4][0]           \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 attention_scores[5][0]           \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 attention_scores[6][0]           \n",
            "                                                                 lambda_13[0][0]                  \n",
            "                                                                 attention_scores[7][0]           \n",
            "                                                                 lambda_15[0][0]                  \n",
            "                                                                 attention_scores[8][0]           \n",
            "                                                                 lambda_17[0][0]                  \n",
            "                                                                 attention_scores[9][0]           \n",
            "                                                                 lambda_19[0][0]                  \n",
            "                                                                 attention_scores[10][0]          \n",
            "                                                                 lambda_21[0][0]                  \n",
            "                                                                 attention_scores[11][0]          \n",
            "                                                                 lambda_23[0][0]                  \n",
            "                                                                 attention_scores[12][0]          \n",
            "                                                                 lambda_25[0][0]                  \n",
            "                                                                 attention_scores[13][0]          \n",
            "                                                                 lambda_27[0][0]                  \n",
            "                                                                 attention_scores[14][0]          \n",
            "                                                                 lambda_29[0][0]                  \n",
            "                                                                 attention_scores[15][0]          \n",
            "                                                                 lambda_31[0][0]                  \n",
            "                                                                 attention_scores[16][0]          \n",
            "                                                                 lambda_33[0][0]                  \n",
            "                                                                 attention_scores[17][0]          \n",
            "                                                                 lambda_35[0][0]                  \n",
            "                                                                 attention_scores[18][0]          \n",
            "                                                                 lambda_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder (LSTM)                  [(None, 256), (None, 394240      dot[0][0]                        \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot[1][0]                        \n",
            "                                                                 decoder[0][0]                    \n",
            "                                                                 decoder[0][2]                    \n",
            "                                                                 dot[2][0]                        \n",
            "                                                                 decoder[1][0]                    \n",
            "                                                                 decoder[1][2]                    \n",
            "                                                                 dot[3][0]                        \n",
            "                                                                 decoder[2][0]                    \n",
            "                                                                 decoder[2][2]                    \n",
            "                                                                 dot[4][0]                        \n",
            "                                                                 decoder[3][0]                    \n",
            "                                                                 decoder[3][2]                    \n",
            "                                                                 dot[5][0]                        \n",
            "                                                                 decoder[4][0]                    \n",
            "                                                                 decoder[4][2]                    \n",
            "                                                                 dot[6][0]                        \n",
            "                                                                 decoder[5][0]                    \n",
            "                                                                 decoder[5][2]                    \n",
            "                                                                 dot[7][0]                        \n",
            "                                                                 decoder[6][0]                    \n",
            "                                                                 decoder[6][2]                    \n",
            "                                                                 dot[8][0]                        \n",
            "                                                                 decoder[7][0]                    \n",
            "                                                                 decoder[7][2]                    \n",
            "                                                                 dot[9][0]                        \n",
            "                                                                 decoder[8][0]                    \n",
            "                                                                 decoder[8][2]                    \n",
            "                                                                 dot[10][0]                       \n",
            "                                                                 decoder[9][0]                    \n",
            "                                                                 decoder[9][2]                    \n",
            "                                                                 dot[11][0]                       \n",
            "                                                                 decoder[10][0]                   \n",
            "                                                                 decoder[10][2]                   \n",
            "                                                                 dot[12][0]                       \n",
            "                                                                 decoder[11][0]                   \n",
            "                                                                 decoder[11][2]                   \n",
            "                                                                 dot[13][0]                       \n",
            "                                                                 decoder[12][0]                   \n",
            "                                                                 decoder[12][2]                   \n",
            "                                                                 dot[14][0]                       \n",
            "                                                                 decoder[13][0]                   \n",
            "                                                                 decoder[13][2]                   \n",
            "                                                                 dot[15][0]                       \n",
            "                                                                 decoder[14][0]                   \n",
            "                                                                 decoder[14][2]                   \n",
            "                                                                 dot[16][0]                       \n",
            "                                                                 decoder[15][0]                   \n",
            "                                                                 decoder[15][2]                   \n",
            "                                                                 dot[17][0]                       \n",
            "                                                                 decoder[16][0]                   \n",
            "                                                                 decoder[16][2]                   \n",
            "                                                                 dot[18][0]                       \n",
            "                                                                 decoder[17][0]                   \n",
            "                                                                 decoder[17][2]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_20 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_22 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_26 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_27 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_28 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_29 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_30 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_31 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_32 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_34 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_36 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_37 (Lambda)              (None, 20, 128)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 23)           5911        decoder[0][0]                    \n",
            "                                                                 decoder[1][0]                    \n",
            "                                                                 decoder[2][0]                    \n",
            "                                                                 decoder[3][0]                    \n",
            "                                                                 decoder[4][0]                    \n",
            "                                                                 decoder[5][0]                    \n",
            "                                                                 decoder[6][0]                    \n",
            "                                                                 decoder[7][0]                    \n",
            "                                                                 decoder[8][0]                    \n",
            "                                                                 decoder[9][0]                    \n",
            "                                                                 decoder[10][0]                   \n",
            "                                                                 decoder[11][0]                   \n",
            "                                                                 decoder[12][0]                   \n",
            "                                                                 decoder[13][0]                   \n",
            "                                                                 decoder[14][0]                   \n",
            "                                                                 decoder[15][0]                   \n",
            "                                                                 decoder[16][0]                   \n",
            "                                                                 decoder[17][0]                   \n",
            "                                                                 decoder[18][0]                   \n",
            "==================================================================================================\n",
            "Total params: 602,668\n",
            "Trainable params: 602,668\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZSNlYhLcGvA"
      },
      "source": [
        "opt = Adam(lr= 0.01, clipvalue=0.5)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eZUIvVZcOf3"
      },
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Y.swapaxes(0,1))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzROwVTecQZ2",
        "outputId": "5d153ae9-239c-4815-901d-3fdb511c963d"
      },
      "source": [
        "model.fit([X, s0, c0], outputs, epochs=20, batch_size=128)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "86/86 [==============================] - 53s 71ms/step - loss: 31.8061 - dense_loss: 0.7196 - dense_1_loss: 0.4103 - dense_2_loss: 0.3643 - dense_3_loss: 0.3814 - dense_4_loss: 0.4236 - dense_5_loss: 0.4944 - dense_6_loss: 0.5865 - dense_7_loss: 0.7260 - dense_8_loss: 0.9139 - dense_9_loss: 1.2030 - dense_10_loss: 1.5927 - dense_11_loss: 2.0881 - dense_12_loss: 2.6302 - dense_13_loss: 3.0276 - dense_14_loss: 3.2711 - dense_15_loss: 3.2390 - dense_16_loss: 3.4607 - dense_17_loss: 3.1554 - dense_18_loss: 3.1184 - dense_accuracy: 0.9418 - dense_1_accuracy: 0.9416 - dense_2_accuracy: 0.9412 - dense_3_accuracy: 0.9402 - dense_4_accuracy: 0.9385 - dense_5_accuracy: 0.9283 - dense_6_accuracy: 0.8918 - dense_7_accuracy: 0.8751 - dense_8_accuracy: 0.8476 - dense_9_accuracy: 0.7881 - dense_10_accuracy: 0.6887 - dense_11_accuracy: 0.5505 - dense_12_accuracy: 0.3450 - dense_13_accuracy: 0.1850 - dense_14_accuracy: 0.0938 - dense_15_accuracy: 0.0926 - dense_16_accuracy: 0.0316 - dense_17_accuracy: 0.1381 - dense_18_accuracy: 0.0664\n",
            "Epoch 2/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 21.0661 - dense_loss: 0.0217 - dense_1_loss: 0.0045 - dense_2_loss: 0.0092 - dense_3_loss: 0.0156 - dense_4_loss: 0.0211 - dense_5_loss: 0.0394 - dense_6_loss: 0.0814 - dense_7_loss: 0.1656 - dense_8_loss: 0.3168 - dense_9_loss: 0.5477 - dense_10_loss: 0.9211 - dense_11_loss: 1.4284 - dense_12_loss: 2.0179 - dense_13_loss: 2.4779 - dense_14_loss: 2.6603 - dense_15_loss: 2.6746 - dense_16_loss: 2.9187 - dense_17_loss: 2.4717 - dense_18_loss: 2.2726 - dense_accuracy: 0.9996 - dense_1_accuracy: 0.9992 - dense_2_accuracy: 0.9986 - dense_3_accuracy: 0.9974 - dense_4_accuracy: 0.9961 - dense_5_accuracy: 0.9924 - dense_6_accuracy: 0.9840 - dense_7_accuracy: 0.9660 - dense_8_accuracy: 0.9324 - dense_9_accuracy: 0.8679 - dense_10_accuracy: 0.7667 - dense_11_accuracy: 0.6208 - dense_12_accuracy: 0.4214 - dense_13_accuracy: 0.2670 - dense_14_accuracy: 0.1945 - dense_15_accuracy: 0.2265 - dense_16_accuracy: 0.0731 - dense_17_accuracy: 0.2988 - dense_18_accuracy: 0.1816\n",
            "Epoch 3/20\n",
            "86/86 [==============================] - 6s 75ms/step - loss: 19.9660 - dense_loss: 0.0080 - dense_1_loss: 0.0031 - dense_2_loss: 0.0046 - dense_3_loss: 0.0072 - dense_4_loss: 0.0129 - dense_5_loss: 0.0414 - dense_6_loss: 0.0782 - dense_7_loss: 0.1458 - dense_8_loss: 0.2792 - dense_9_loss: 0.5241 - dense_10_loss: 0.8615 - dense_11_loss: 1.3384 - dense_12_loss: 1.8778 - dense_13_loss: 2.3464 - dense_14_loss: 2.5394 - dense_15_loss: 2.5532 - dense_16_loss: 2.7929 - dense_17_loss: 2.4201 - dense_18_loss: 2.1320 - dense_accuracy: 0.9998 - dense_1_accuracy: 0.9995 - dense_2_accuracy: 0.9992 - dense_3_accuracy: 0.9985 - dense_4_accuracy: 0.9975 - dense_5_accuracy: 0.9921 - dense_6_accuracy: 0.9838 - dense_7_accuracy: 0.9656 - dense_8_accuracy: 0.9352 - dense_9_accuracy: 0.8661 - dense_10_accuracy: 0.7671 - dense_11_accuracy: 0.6225 - dense_12_accuracy: 0.4426 - dense_13_accuracy: 0.2866 - dense_14_accuracy: 0.2020 - dense_15_accuracy: 0.2016 - dense_16_accuracy: 0.1369 - dense_17_accuracy: 0.2869 - dense_18_accuracy: 0.2390\n",
            "Epoch 4/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 18.9960 - dense_loss: 0.0061 - dense_1_loss: 0.0022 - dense_2_loss: 0.0052 - dense_3_loss: 0.0095 - dense_4_loss: 0.0151 - dense_5_loss: 0.0360 - dense_6_loss: 0.0712 - dense_7_loss: 0.1352 - dense_8_loss: 0.2538 - dense_9_loss: 0.4720 - dense_10_loss: 0.8038 - dense_11_loss: 1.2636 - dense_12_loss: 1.7969 - dense_13_loss: 2.2325 - dense_14_loss: 2.4962 - dense_15_loss: 2.4356 - dense_16_loss: 2.6426 - dense_17_loss: 2.3383 - dense_18_loss: 1.9802 - dense_accuracy: 0.9997 - dense_1_accuracy: 0.9997 - dense_2_accuracy: 0.9990 - dense_3_accuracy: 0.9979 - dense_4_accuracy: 0.9966 - dense_5_accuracy: 0.9918 - dense_6_accuracy: 0.9849 - dense_7_accuracy: 0.9678 - dense_8_accuracy: 0.9394 - dense_9_accuracy: 0.8765 - dense_10_accuracy: 0.7775 - dense_11_accuracy: 0.6359 - dense_12_accuracy: 0.4594 - dense_13_accuracy: 0.3027 - dense_14_accuracy: 0.1899 - dense_15_accuracy: 0.2116 - dense_16_accuracy: 0.1671 - dense_17_accuracy: 0.3173 - dense_18_accuracy: 0.2893\n",
            "Epoch 5/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 17.0154 - dense_loss: 0.0044 - dense_1_loss: 0.0013 - dense_2_loss: 0.0056 - dense_3_loss: 0.0077 - dense_4_loss: 0.0128 - dense_5_loss: 0.0358 - dense_6_loss: 0.0653 - dense_7_loss: 0.1243 - dense_8_loss: 0.2365 - dense_9_loss: 0.4243 - dense_10_loss: 0.7344 - dense_11_loss: 1.1474 - dense_12_loss: 1.5954 - dense_13_loss: 1.9883 - dense_14_loss: 2.2309 - dense_15_loss: 2.2168 - dense_16_loss: 2.3988 - dense_17_loss: 2.0893 - dense_18_loss: 1.6964 - dense_accuracy: 0.9998 - dense_1_accuracy: 0.9998 - dense_2_accuracy: 0.9989 - dense_3_accuracy: 0.9982 - dense_4_accuracy: 0.9966 - dense_5_accuracy: 0.9919 - dense_6_accuracy: 0.9835 - dense_7_accuracy: 0.9679 - dense_8_accuracy: 0.9389 - dense_9_accuracy: 0.8796 - dense_10_accuracy: 0.7809 - dense_11_accuracy: 0.6499 - dense_12_accuracy: 0.4930 - dense_13_accuracy: 0.3586 - dense_14_accuracy: 0.2462 - dense_15_accuracy: 0.2533 - dense_16_accuracy: 0.2176 - dense_17_accuracy: 0.3511 - dense_18_accuracy: 0.3927\n",
            "Epoch 6/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 15.3806 - dense_loss: 0.0051 - dense_1_loss: 0.0010 - dense_2_loss: 0.0051 - dense_3_loss: 0.0073 - dense_4_loss: 0.0188 - dense_5_loss: 0.0368 - dense_6_loss: 0.0664 - dense_7_loss: 0.1254 - dense_8_loss: 0.2285 - dense_9_loss: 0.4070 - dense_10_loss: 0.6857 - dense_11_loss: 1.0421 - dense_12_loss: 1.4475 - dense_13_loss: 1.8233 - dense_14_loss: 2.0149 - dense_15_loss: 1.9628 - dense_16_loss: 2.1533 - dense_17_loss: 1.8900 - dense_18_loss: 1.4597 - dense_accuracy: 0.9999 - dense_1_accuracy: 0.9998 - dense_2_accuracy: 0.9992 - dense_3_accuracy: 0.9984 - dense_4_accuracy: 0.9956 - dense_5_accuracy: 0.9902 - dense_6_accuracy: 0.9838 - dense_7_accuracy: 0.9669 - dense_8_accuracy: 0.9395 - dense_9_accuracy: 0.8797 - dense_10_accuracy: 0.7965 - dense_11_accuracy: 0.6718 - dense_12_accuracy: 0.5352 - dense_13_accuracy: 0.3932 - dense_14_accuracy: 0.3073 - dense_15_accuracy: 0.3263 - dense_16_accuracy: 0.2693 - dense_17_accuracy: 0.3808 - dense_18_accuracy: 0.4696\n",
            "Epoch 7/20\n",
            "86/86 [==============================] - 6s 69ms/step - loss: 13.9299 - dense_loss: 0.0059 - dense_1_loss: 0.0012 - dense_2_loss: 0.0052 - dense_3_loss: 0.0074 - dense_4_loss: 0.0146 - dense_5_loss: 0.0325 - dense_6_loss: 0.0652 - dense_7_loss: 0.1219 - dense_8_loss: 0.2147 - dense_9_loss: 0.3867 - dense_10_loss: 0.6246 - dense_11_loss: 0.9677 - dense_12_loss: 1.3436 - dense_13_loss: 1.6561 - dense_14_loss: 1.8390 - dense_15_loss: 1.8194 - dense_16_loss: 1.9562 - dense_17_loss: 1.6701 - dense_18_loss: 1.1981 - dense_accuracy: 0.9997 - dense_1_accuracy: 0.9997 - dense_2_accuracy: 0.9989 - dense_3_accuracy: 0.9984 - dense_4_accuracy: 0.9957 - dense_5_accuracy: 0.9916 - dense_6_accuracy: 0.9828 - dense_7_accuracy: 0.9669 - dense_8_accuracy: 0.9401 - dense_9_accuracy: 0.8826 - dense_10_accuracy: 0.8080 - dense_11_accuracy: 0.6930 - dense_12_accuracy: 0.5585 - dense_13_accuracy: 0.4502 - dense_14_accuracy: 0.3643 - dense_15_accuracy: 0.3555 - dense_16_accuracy: 0.3404 - dense_17_accuracy: 0.4225 - dense_18_accuracy: 0.5741\n",
            "Epoch 8/20\n",
            "86/86 [==============================] - 6s 71ms/step - loss: 11.6486 - dense_loss: 0.0040 - dense_1_loss: 0.0013 - dense_2_loss: 0.0040 - dense_3_loss: 0.0051 - dense_4_loss: 0.0094 - dense_5_loss: 0.0269 - dense_6_loss: 0.0564 - dense_7_loss: 0.1116 - dense_8_loss: 0.1847 - dense_9_loss: 0.3275 - dense_10_loss: 0.5410 - dense_11_loss: 0.8314 - dense_12_loss: 1.1407 - dense_13_loss: 1.4222 - dense_14_loss: 1.5881 - dense_15_loss: 1.5772 - dense_16_loss: 1.6408 - dense_17_loss: 1.3169 - dense_18_loss: 0.8593 - dense_accuracy: 0.9998 - dense_1_accuracy: 0.9997 - dense_2_accuracy: 0.9991 - dense_3_accuracy: 0.9987 - dense_4_accuracy: 0.9974 - dense_5_accuracy: 0.9933 - dense_6_accuracy: 0.9853 - dense_7_accuracy: 0.9688 - dense_8_accuracy: 0.9418 - dense_9_accuracy: 0.8984 - dense_10_accuracy: 0.8275 - dense_11_accuracy: 0.7328 - dense_12_accuracy: 0.6299 - dense_13_accuracy: 0.5209 - dense_14_accuracy: 0.4456 - dense_15_accuracy: 0.4378 - dense_16_accuracy: 0.4514 - dense_17_accuracy: 0.5442 - dense_18_accuracy: 0.7072\n",
            "Epoch 9/20\n",
            "86/86 [==============================] - 6s 72ms/step - loss: 10.1916 - dense_loss: 0.0039 - dense_1_loss: 0.0012 - dense_2_loss: 0.0030 - dense_3_loss: 0.0064 - dense_4_loss: 0.0126 - dense_5_loss: 0.0300 - dense_6_loss: 0.0579 - dense_7_loss: 0.1026 - dense_8_loss: 0.1831 - dense_9_loss: 0.3084 - dense_10_loss: 0.5209 - dense_11_loss: 0.7788 - dense_12_loss: 1.0609 - dense_13_loss: 1.3024 - dense_14_loss: 1.4416 - dense_15_loss: 1.3520 - dense_16_loss: 1.3834 - dense_17_loss: 1.0375 - dense_18_loss: 0.6051 - dense_accuracy: 0.9999 - dense_1_accuracy: 0.9998 - dense_2_accuracy: 0.9992 - dense_3_accuracy: 0.9983 - dense_4_accuracy: 0.9967 - dense_5_accuracy: 0.9913 - dense_6_accuracy: 0.9843 - dense_7_accuracy: 0.9697 - dense_8_accuracy: 0.9420 - dense_9_accuracy: 0.9018 - dense_10_accuracy: 0.8263 - dense_11_accuracy: 0.7426 - dense_12_accuracy: 0.6458 - dense_13_accuracy: 0.5517 - dense_14_accuracy: 0.5047 - dense_15_accuracy: 0.5484 - dense_16_accuracy: 0.5487 - dense_17_accuracy: 0.6644 - dense_18_accuracy: 0.8230\n",
            "Epoch 10/20\n",
            "86/86 [==============================] - 6s 71ms/step - loss: 8.6439 - dense_loss: 0.0037 - dense_1_loss: 0.0014 - dense_2_loss: 0.0043 - dense_3_loss: 0.0087 - dense_4_loss: 0.0122 - dense_5_loss: 0.0278 - dense_6_loss: 0.0444 - dense_7_loss: 0.0849 - dense_8_loss: 0.1641 - dense_9_loss: 0.2777 - dense_10_loss: 0.4736 - dense_11_loss: 0.7170 - dense_12_loss: 0.9786 - dense_13_loss: 1.1675 - dense_14_loss: 1.2529 - dense_15_loss: 1.1542 - dense_16_loss: 1.0771 - dense_17_loss: 0.7225 - dense_18_loss: 0.4713 - dense_accuracy: 0.9999 - dense_1_accuracy: 0.9998 - dense_2_accuracy: 0.9989 - dense_3_accuracy: 0.9978 - dense_4_accuracy: 0.9969 - dense_5_accuracy: 0.9923 - dense_6_accuracy: 0.9866 - dense_7_accuracy: 0.9729 - dense_8_accuracy: 0.9467 - dense_9_accuracy: 0.9081 - dense_10_accuracy: 0.8443 - dense_11_accuracy: 0.7602 - dense_12_accuracy: 0.6807 - dense_13_accuracy: 0.6027 - dense_14_accuracy: 0.5748 - dense_15_accuracy: 0.6241 - dense_16_accuracy: 0.6612 - dense_17_accuracy: 0.7700 - dense_18_accuracy: 0.8531\n",
            "Epoch 11/20\n",
            "86/86 [==============================] - 6s 71ms/step - loss: 8.5340 - dense_loss: 0.0035 - dense_1_loss: 0.0021 - dense_2_loss: 0.0046 - dense_3_loss: 0.0054 - dense_4_loss: 0.0093 - dense_5_loss: 0.0191 - dense_6_loss: 0.0409 - dense_7_loss: 0.0907 - dense_8_loss: 0.1699 - dense_9_loss: 0.3072 - dense_10_loss: 0.5019 - dense_11_loss: 0.7315 - dense_12_loss: 0.9920 - dense_13_loss: 1.1826 - dense_14_loss: 1.2369 - dense_15_loss: 1.1065 - dense_16_loss: 0.9970 - dense_17_loss: 0.6827 - dense_18_loss: 0.4500 - dense_accuracy: 0.9998 - dense_1_accuracy: 0.9997 - dense_2_accuracy: 0.9993 - dense_3_accuracy: 0.9988 - dense_4_accuracy: 0.9978 - dense_5_accuracy: 0.9956 - dense_6_accuracy: 0.9889 - dense_7_accuracy: 0.9736 - dense_8_accuracy: 0.9480 - dense_9_accuracy: 0.9006 - dense_10_accuracy: 0.8331 - dense_11_accuracy: 0.7464 - dense_12_accuracy: 0.6574 - dense_13_accuracy: 0.5856 - dense_14_accuracy: 0.5735 - dense_15_accuracy: 0.6335 - dense_16_accuracy: 0.6789 - dense_17_accuracy: 0.7805 - dense_18_accuracy: 0.8586\n",
            "Epoch 12/20\n",
            "86/86 [==============================] - 6s 71ms/step - loss: 6.0454 - dense_loss: 0.0032 - dense_1_loss: 0.0019 - dense_2_loss: 0.0037 - dense_3_loss: 0.0082 - dense_4_loss: 0.0123 - dense_5_loss: 0.0255 - dense_6_loss: 0.0451 - dense_7_loss: 0.0832 - dense_8_loss: 0.1362 - dense_9_loss: 0.2311 - dense_10_loss: 0.3918 - dense_11_loss: 0.5733 - dense_12_loss: 0.7577 - dense_13_loss: 0.8927 - dense_14_loss: 0.9180 - dense_15_loss: 0.7664 - dense_16_loss: 0.6211 - dense_17_loss: 0.3375 - dense_18_loss: 0.2363 - dense_accuracy: 0.9998 - dense_1_accuracy: 0.9998 - dense_2_accuracy: 0.9991 - dense_3_accuracy: 0.9976 - dense_4_accuracy: 0.9960 - dense_5_accuracy: 0.9933 - dense_6_accuracy: 0.9854 - dense_7_accuracy: 0.9729 - dense_8_accuracy: 0.9524 - dense_9_accuracy: 0.9195 - dense_10_accuracy: 0.8653 - dense_11_accuracy: 0.8037 - dense_12_accuracy: 0.7476 - dense_13_accuracy: 0.6948 - dense_14_accuracy: 0.7014 - dense_15_accuracy: 0.7578 - dense_16_accuracy: 0.8194 - dense_17_accuracy: 0.9060 - dense_18_accuracy: 0.9372\n",
            "Epoch 13/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 5.3847 - dense_loss: 0.0025 - dense_1_loss: 0.0021 - dense_2_loss: 0.0064 - dense_3_loss: 0.0064 - dense_4_loss: 0.0106 - dense_5_loss: 0.0216 - dense_6_loss: 0.0403 - dense_7_loss: 0.0744 - dense_8_loss: 0.1334 - dense_9_loss: 0.2141 - dense_10_loss: 0.3493 - dense_11_loss: 0.5264 - dense_12_loss: 0.6935 - dense_13_loss: 0.8214 - dense_14_loss: 0.8144 - dense_15_loss: 0.6866 - dense_16_loss: 0.5016 - dense_17_loss: 0.2892 - dense_18_loss: 0.1906 - dense_accuracy: 0.9999 - dense_1_accuracy: 0.9995 - dense_2_accuracy: 0.9986 - dense_3_accuracy: 0.9977 - dense_4_accuracy: 0.9970 - dense_5_accuracy: 0.9932 - dense_6_accuracy: 0.9874 - dense_7_accuracy: 0.9749 - dense_8_accuracy: 0.9583 - dense_9_accuracy: 0.9313 - dense_10_accuracy: 0.8824 - dense_11_accuracy: 0.8177 - dense_12_accuracy: 0.7677 - dense_13_accuracy: 0.7204 - dense_14_accuracy: 0.7414 - dense_15_accuracy: 0.7922 - dense_16_accuracy: 0.8570 - dense_17_accuracy: 0.9224 - dense_18_accuracy: 0.9488\n",
            "Epoch 14/20\n",
            "86/86 [==============================] - 6s 72ms/step - loss: 4.8111 - dense_loss: 0.0033 - dense_1_loss: 0.0012 - dense_2_loss: 0.0030 - dense_3_loss: 0.0078 - dense_4_loss: 0.0117 - dense_5_loss: 0.0208 - dense_6_loss: 0.0386 - dense_7_loss: 0.0746 - dense_8_loss: 0.1162 - dense_9_loss: 0.1911 - dense_10_loss: 0.3170 - dense_11_loss: 0.4886 - dense_12_loss: 0.6235 - dense_13_loss: 0.7345 - dense_14_loss: 0.7138 - dense_15_loss: 0.5874 - dense_16_loss: 0.4317 - dense_17_loss: 0.2596 - dense_18_loss: 0.1868 - dense_accuracy: 0.9998 - dense_1_accuracy: 0.9998 - dense_2_accuracy: 0.9991 - dense_3_accuracy: 0.9980 - dense_4_accuracy: 0.9964 - dense_5_accuracy: 0.9928 - dense_6_accuracy: 0.9879 - dense_7_accuracy: 0.9769 - dense_8_accuracy: 0.9618 - dense_9_accuracy: 0.9350 - dense_10_accuracy: 0.8923 - dense_11_accuracy: 0.8369 - dense_12_accuracy: 0.7923 - dense_13_accuracy: 0.7501 - dense_14_accuracy: 0.7731 - dense_15_accuracy: 0.8150 - dense_16_accuracy: 0.8773 - dense_17_accuracy: 0.9256 - dense_18_accuracy: 0.9446\n",
            "Epoch 15/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 4.2603 - dense_loss: 0.0029 - dense_1_loss: 0.0016 - dense_2_loss: 0.0028 - dense_3_loss: 0.0049 - dense_4_loss: 0.0068 - dense_5_loss: 0.0163 - dense_6_loss: 0.0366 - dense_7_loss: 0.0627 - dense_8_loss: 0.1070 - dense_9_loss: 0.1750 - dense_10_loss: 0.2933 - dense_11_loss: 0.4353 - dense_12_loss: 0.5633 - dense_13_loss: 0.6508 - dense_14_loss: 0.6467 - dense_15_loss: 0.5070 - dense_16_loss: 0.3648 - dense_17_loss: 0.2240 - dense_18_loss: 0.1585 - dense_accuracy: 0.9997 - dense_1_accuracy: 0.9996 - dense_2_accuracy: 0.9994 - dense_3_accuracy: 0.9986 - dense_4_accuracy: 0.9981 - dense_5_accuracy: 0.9947 - dense_6_accuracy: 0.9882 - dense_7_accuracy: 0.9795 - dense_8_accuracy: 0.9649 - dense_9_accuracy: 0.9430 - dense_10_accuracy: 0.9003 - dense_11_accuracy: 0.8510 - dense_12_accuracy: 0.8106 - dense_13_accuracy: 0.7801 - dense_14_accuracy: 0.7864 - dense_15_accuracy: 0.8453 - dense_16_accuracy: 0.8931 - dense_17_accuracy: 0.9302 - dense_18_accuracy: 0.9572\n",
            "Epoch 16/20\n",
            "86/86 [==============================] - 6s 71ms/step - loss: 3.7255 - dense_loss: 0.0014 - dense_1_loss: 4.6021e-04 - dense_2_loss: 0.0017 - dense_3_loss: 0.0035 - dense_4_loss: 0.0084 - dense_5_loss: 0.0191 - dense_6_loss: 0.0339 - dense_7_loss: 0.0593 - dense_8_loss: 0.1055 - dense_9_loss: 0.1886 - dense_10_loss: 0.2868 - dense_11_loss: 0.4074 - dense_12_loss: 0.5065 - dense_13_loss: 0.5757 - dense_14_loss: 0.5277 - dense_15_loss: 0.4096 - dense_16_loss: 0.2878 - dense_17_loss: 0.1727 - dense_18_loss: 0.1293 - dense_accuracy: 0.9999 - dense_1_accuracy: 0.9999 - dense_2_accuracy: 0.9996 - dense_3_accuracy: 0.9988 - dense_4_accuracy: 0.9977 - dense_5_accuracy: 0.9940 - dense_6_accuracy: 0.9893 - dense_7_accuracy: 0.9791 - dense_8_accuracy: 0.9622 - dense_9_accuracy: 0.9362 - dense_10_accuracy: 0.9015 - dense_11_accuracy: 0.8618 - dense_12_accuracy: 0.8306 - dense_13_accuracy: 0.8125 - dense_14_accuracy: 0.8309 - dense_15_accuracy: 0.8789 - dense_16_accuracy: 0.9225 - dense_17_accuracy: 0.9503 - dense_18_accuracy: 0.9638\n",
            "Epoch 17/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 4.6019 - dense_loss: 0.0020 - dense_1_loss: 9.5359e-04 - dense_2_loss: 0.0050 - dense_3_loss: 0.0068 - dense_4_loss: 0.0094 - dense_5_loss: 0.0230 - dense_6_loss: 0.0410 - dense_7_loss: 0.0804 - dense_8_loss: 0.1335 - dense_9_loss: 0.2193 - dense_10_loss: 0.3393 - dense_11_loss: 0.4827 - dense_12_loss: 0.6154 - dense_13_loss: 0.6835 - dense_14_loss: 0.6434 - dense_15_loss: 0.5041 - dense_16_loss: 0.3811 - dense_17_loss: 0.2608 - dense_18_loss: 0.1702 - dense_accuracy: 0.9999 - dense_1_accuracy: 0.9999 - dense_2_accuracy: 0.9991 - dense_3_accuracy: 0.9978 - dense_4_accuracy: 0.9968 - dense_5_accuracy: 0.9924 - dense_6_accuracy: 0.9872 - dense_7_accuracy: 0.9740 - dense_8_accuracy: 0.9532 - dense_9_accuracy: 0.9264 - dense_10_accuracy: 0.8838 - dense_11_accuracy: 0.8316 - dense_12_accuracy: 0.7888 - dense_13_accuracy: 0.7626 - dense_14_accuracy: 0.7898 - dense_15_accuracy: 0.8379 - dense_16_accuracy: 0.8875 - dense_17_accuracy: 0.9185 - dense_18_accuracy: 0.9517\n",
            "Epoch 18/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 2.6570 - dense_loss: 0.0017 - dense_1_loss: 7.8583e-04 - dense_2_loss: 0.0028 - dense_3_loss: 0.0053 - dense_4_loss: 0.0092 - dense_5_loss: 0.0170 - dense_6_loss: 0.0351 - dense_7_loss: 0.0544 - dense_8_loss: 0.0867 - dense_9_loss: 0.1473 - dense_10_loss: 0.2141 - dense_11_loss: 0.2992 - dense_12_loss: 0.3746 - dense_13_loss: 0.4202 - dense_14_loss: 0.3758 - dense_15_loss: 0.2696 - dense_16_loss: 0.1767 - dense_17_loss: 0.0961 - dense_18_loss: 0.0703 - dense_accuracy: 0.9999 - dense_1_accuracy: 0.9998 - dense_2_accuracy: 0.9993 - dense_3_accuracy: 0.9985 - dense_4_accuracy: 0.9969 - dense_5_accuracy: 0.9935 - dense_6_accuracy: 0.9876 - dense_7_accuracy: 0.9802 - dense_8_accuracy: 0.9694 - dense_9_accuracy: 0.9523 - dense_10_accuracy: 0.9323 - dense_11_accuracy: 0.9066 - dense_12_accuracy: 0.8824 - dense_13_accuracy: 0.8643 - dense_14_accuracy: 0.8810 - dense_15_accuracy: 0.9199 - dense_16_accuracy: 0.9534 - dense_17_accuracy: 0.9776 - dense_18_accuracy: 0.9818\n",
            "Epoch 19/20\n",
            "86/86 [==============================] - 6s 70ms/step - loss: 2.7943 - dense_loss: 0.0022 - dense_1_loss: 0.0012 - dense_2_loss: 0.0034 - dense_3_loss: 0.0056 - dense_4_loss: 0.0087 - dense_5_loss: 0.0146 - dense_6_loss: 0.0274 - dense_7_loss: 0.0453 - dense_8_loss: 0.0851 - dense_9_loss: 0.1360 - dense_10_loss: 0.2216 - dense_11_loss: 0.3198 - dense_12_loss: 0.3908 - dense_13_loss: 0.4409 - dense_14_loss: 0.3944 - dense_15_loss: 0.2980 - dense_16_loss: 0.1954 - dense_17_loss: 0.1219 - dense_18_loss: 0.0819 - dense_accuracy: 0.9997 - dense_1_accuracy: 0.9997 - dense_2_accuracy: 0.9989 - dense_3_accuracy: 0.9985 - dense_4_accuracy: 0.9964 - dense_5_accuracy: 0.9950 - dense_6_accuracy: 0.9902 - dense_7_accuracy: 0.9854 - dense_8_accuracy: 0.9682 - dense_9_accuracy: 0.9537 - dense_10_accuracy: 0.9259 - dense_11_accuracy: 0.8889 - dense_12_accuracy: 0.8725 - dense_13_accuracy: 0.8549 - dense_14_accuracy: 0.8738 - dense_15_accuracy: 0.9114 - dense_16_accuracy: 0.9450 - dense_17_accuracy: 0.9677 - dense_18_accuracy: 0.9778\n",
            "Epoch 20/20\n",
            "86/86 [==============================] - 6s 71ms/step - loss: 2.5691 - dense_loss: 0.0012 - dense_1_loss: 7.8611e-04 - dense_2_loss: 0.0034 - dense_3_loss: 0.0075 - dense_4_loss: 0.0131 - dense_5_loss: 0.0200 - dense_6_loss: 0.0326 - dense_7_loss: 0.0568 - dense_8_loss: 0.0944 - dense_9_loss: 0.1371 - dense_10_loss: 0.2083 - dense_11_loss: 0.2853 - dense_12_loss: 0.3587 - dense_13_loss: 0.3760 - dense_14_loss: 0.3519 - dense_15_loss: 0.2615 - dense_16_loss: 0.1856 - dense_17_loss: 0.1020 - dense_18_loss: 0.0730 - dense_accuracy: 0.9999 - dense_1_accuracy: 0.9999 - dense_2_accuracy: 0.9987 - dense_3_accuracy: 0.9972 - dense_4_accuracy: 0.9953 - dense_5_accuracy: 0.9930 - dense_6_accuracy: 0.9877 - dense_7_accuracy: 0.9786 - dense_8_accuracy: 0.9691 - dense_9_accuracy: 0.9537 - dense_10_accuracy: 0.9314 - dense_11_accuracy: 0.9074 - dense_12_accuracy: 0.8819 - dense_13_accuracy: 0.8780 - dense_14_accuracy: 0.8867 - dense_15_accuracy: 0.9208 - dense_16_accuracy: 0.9484 - dense_17_accuracy: 0.9714 - dense_18_accuracy: 0.9802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3069f83e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C2RET9GwXDh"
      },
      "source": [
        "# Thai-Script to Roman-Script Translation\n",
        "* Task 4: Test your model on 5 examples of your choice including your name! (1 point)\n",
        "* Task 5: Show your visualization of attention scores on one of your example (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gON7T2xVwXDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed028c0-17db-48ae-f7e2-4dfaef75dd9b"
      },
      "source": [
        "#task 4\n",
        "#fill your code here\n",
        "test_x = ['นราวิชญ์', 'รวิภาส', 'นวพร', 'พงศ์ศิริ', 'ภูมิไผท']\n",
        "\n",
        "X_test = []\n",
        "for line in test_x:\n",
        "    temp=[]\n",
        "    for char in line:\n",
        "        temp.append(char_to_ix[char])\n",
        "    X_test.append(temp)\n",
        "\n",
        "X_test = pad_sequences(X_test,maxlen=maxlen)\n",
        "X_test = to_categorical(X_test,vocab_size)\n",
        "X_test = X_test.reshape(len(test_x),maxlen ,vocab_size)\n",
        "\n",
        "print(X_test.shape)\n",
        "\n",
        "s0 = np.zeros((len(test_x), n_s))\n",
        "c0 = np.zeros((len(test_x), n_s))\n",
        "y_pred = model.predict([X_test, s0, c0])\n",
        "len(y_pred), y_pred[0].shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 20, 65)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19, (5, 23))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB7LYnJdVAkc"
      },
      "source": [
        "import numpy as np\n",
        "y_pred = np.array(y_pred)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a876LiMPVQfW",
        "outputId": "ff3f1be1-76ee-4704-fd43-423b0f0e6606"
      },
      "source": [
        "for i in range(5):\n",
        "  y_each = ''.join([ix_to_output_char[np.argmax(each)] for each in y_pred[:,i,:] if np.argmax(each) != 0])\n",
        "  print(test_x[i], y_each)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "นราวิชญ์ nnarawit\n",
            "รวิภาส rawiphat\n",
            "นวพร nawaphon\n",
            "พงศ์ศิริ phongsiri\n",
            "ภูมิไผท phhmaphatt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO8aHkR4t-_v"
      },
      "source": [
        "from tensorflow import convert_to_tensor\n",
        "\n",
        "attention_list = list()\n",
        "def one_step_attention_inference(a, s_prev):\n",
        "    #Fill code here\n",
        "    a_key = Lambda(lambda x: x[:,:,:n_h])(a)\n",
        "    a_val = Lambda(lambda x: x[:,:,n_h:])(a)\n",
        "    # Repeat the decoder hidden state to concat with encoder hidden states\n",
        "    s_prev = repeator(s_prev)\n",
        "    concat = concatenator([a_key,s_prev])\n",
        "    # attention function\n",
        "    e = fattn_1(concat)\n",
        "    energies =fattn_2(e) # (None, 1)\n",
        "    # calculate attention_scores (softmax)\n",
        "    attention_scores = activator(energies) #(None, 1)\n",
        "    attention_list.append(attention_scores)\n",
        "    #calculate a context vector\n",
        "    context = dotor([attention_scores,a_val])\n",
        "\n",
        "    return context # return whatever you need to complete this homework \n",
        "\n",
        "\n",
        "def get_inference_model(X, s0, c0):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_h -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    vocab_size -- size of the input vocab\n",
        "    output_vocab_size -- size of the output vocab\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input of your model\n",
        "    # X = Input(shape=(Tx, vocab_size), name='input_name')\n",
        "    # # Define hidden state and cell state for decoder_LSTM_Cell\n",
        "    # s0 = Input(shape=(n_s,), name='s0')\n",
        "    # c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = list()\n",
        "\n",
        "    #Encoder Bi-LSTM\n",
        "    # h = Bidirectional(LSTM(n_h, return_sequences=True),input_shape=(-1, Tx, n_h*2))(X)\n",
        "    h = encoder_LSTM(X)\n",
        "    #Iterate for Ty steps (Decoding)\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        #Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
        "        context = one_step_attention_inference(h, s)\n",
        "        # Feed the context vector to the decoder LSTM cell\n",
        "        s, _, c = decoder_LSTM_cell(context,initial_state=[s,c])\n",
        "\n",
        "        # Pass the decoder hidden output to the output layer (softmax)\n",
        "        out = output_layer(s)\n",
        "        \n",
        "        # Append an output list with the current output\n",
        "        outputs.append(out)\n",
        "    \n",
        "    #Create model instance\n",
        "    # model = Model(inputs=[X,s0,c0],outputs=outputs)\n",
        "    \n",
        "    return outputs\n",
        "_ = get_inference_model(convert_to_tensor(X_test, dtype=tf.float32), convert_to_tensor(s0, dtype=tf.float32), convert_to_tensor(c0, dtype=tf.float32))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaEGTAM4zZ4t"
      },
      "source": [
        "from tensorflow import reshape\n",
        "attention_tf = reshape(convert_to_tensor(attention_list), (5,20,19))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jUp8ugA0FLv"
      },
      "source": [
        "attention_np = attention_tf.numpy()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajbdDCwbyYO8",
        "outputId": "e2c890b4-fc89-4411-ad0e-94cf4a59e9f1"
      },
      "source": [
        "len(attention_list), attention_list[0].shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19, TensorShape([5, 20, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-mxbsKwXDp"
      },
      "source": [
        "### Plot the attention map\n",
        "* If you need to install thai font: sudo apt install xfonts-thai\n",
        "* this is what your visualization might look like:\n",
        "<img src=\"https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/images/attn_viz_sample.png\"  style=\"width: 350px;\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFGgm0R7Xyp-"
      },
      "source": [
        "!sudo apt install xfonts-thai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewbBSXDV2QSW",
        "outputId": "149798c7-8f2a-479d-c2c3-e1ea18163459"
      },
      "source": [
        "_test_x = ['นราวิชญ์']\n",
        "\n",
        "_X_test = []\n",
        "for line in _test_x:\n",
        "    temp=[]\n",
        "    for char in line:\n",
        "        temp.append(char_to_ix[char])\n",
        "    _X_test.append(temp)\n",
        "\n",
        "_X_test = pad_sequences(_X_test,maxlen=maxlen)\n",
        "_X_test.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8AuxSw95pM7",
        "outputId": "ed8104f0-f2da-48d3-d612-d7441369aae7"
      },
      "source": [
        "attention_np[0].shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh7AXotY5qX_",
        "outputId": "696ae301-298f-42c6-b78b-1f5bb1fae48b"
      },
      "source": [
        "len(english), len(thai)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRL8hHaLwXDq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "f4bcbd95-b0a5-48b5-a5d9-7ed3003d7c6f"
      },
      "source": [
        "#task 5\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family']='TH Sarabun New'  #you can change to other font that works for you\n",
        "#fill your code here\n",
        "\n",
        "\n",
        "english = [ix_to_output_char[np.argmax(each)] for each in y_pred[:,0,:]]\n",
        "thai = [ix_to_char[each] for each in _X_test[0]]\n",
        "fig, ax = plt.subplots(figsize=(10,10))  \n",
        "heatmap = sns.heatmap(\n",
        "    attention_np[0],\n",
        "    xticklabels=english[::-1], \n",
        "    yticklabels=thai[::-1])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6qiasUQ1P6B",
        "outputId": "06de0580-262e-4e15-bee9-4d4c632b46f3"
      },
      "source": [
        "X_test[0].shape"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    }
  ]
}