{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "hw5_thai_skip_gram_homework_for_student_v2020s2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icekang/NLP_2021/blob/main/hw5_thai_skip_gram_homework_for_student_v2020s2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko8K6ntRi-W6"
      },
      "source": [
        "# Homework: Word Embedding\n",
        "\n",
        "In this exercise, you will work on the skip-gram neural network architecture for Word2Vec. You will be using Keras to train your model. \n",
        "\n",
        "You must complete the following tasks:\n",
        "1. Read/clean text files\n",
        "2. Indexing (Assign a number to each word)\n",
        "3. Create skip-grams (inputs for your model)\n",
        "4. Create the skip-gram neural network model\n",
        "5. Visualization\n",
        "6. Evaluation (Using pre-trained, not using pre-trained)\n",
        "    (classify topic from 4 categories) \n",
        "    \n",
        "This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled. If you run this exercise on GCloud using the provided disk image you are all set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw11OhLsi-W8"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import collections\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import GRU, Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense, Masking, Conv1D, Bidirectional\n",
        "from tensorflow.python.keras.layers.merge import Dot\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras.utils.data_utils import get_file\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdYpL3Uyi-XD"
      },
      "source": [
        "# Step 1: Read/clean text files\n",
        "\n",
        "The given code can be used to processed the pre-tokenzied text file from the wikipedia corpus. In your homework, you must replace those text files with raw text files.  You must use your own tokenizer to process your text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wco1eVRVzn6O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "35697b50-cb2b-43f5-ce30-b3a9762272dc"
      },
      "source": [
        "!wget https://www.dropbox.com/s/eexden7246sgfzf/BEST-TrainingSet.zip\n",
        "!wget https://www.dropbox.com/s/n87fiy25f2yc3gt/wiki.zip\n",
        "!unzip wiki.zip\n",
        "\n",
        "!unzip BEST-TrainingSet.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-21 07:00:26--  https://www.dropbox.com/s/eexden7246sgfzf/BEST-TrainingSet.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/eexden7246sgfzf/BEST-TrainingSet.zip [following]\n",
            "--2021-02-21 07:00:26--  https://www.dropbox.com/s/raw/eexden7246sgfzf/BEST-TrainingSet.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc57a9415f9781202ce5776f8af5.dl.dropboxusercontent.com/cd/0/inline/BJXjAp967rZQyBKndDnGmyIVqOIpIU500eUAN2mp7N3XDX5deCjet-Fvn30dMglL52AMmvEA9NbwAqnYLNniUeh7zWmcQGRtm4NudyUS7-s10yFCv6-SLyTICl0sHglwXbA/file# [following]\n",
            "--2021-02-21 07:00:26--  https://uc57a9415f9781202ce5776f8af5.dl.dropboxusercontent.com/cd/0/inline/BJXjAp967rZQyBKndDnGmyIVqOIpIU500eUAN2mp7N3XDX5deCjet-Fvn30dMglL52AMmvEA9NbwAqnYLNniUeh7zWmcQGRtm4NudyUS7-s10yFCv6-SLyTICl0sHglwXbA/file\n",
            "Resolving uc57a9415f9781202ce5776f8af5.dl.dropboxusercontent.com (uc57a9415f9781202ce5776f8af5.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to uc57a9415f9781202ce5776f8af5.dl.dropboxusercontent.com (uc57a9415f9781202ce5776f8af5.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BJW6U4UtgeUaXS9qv-sau6hDPpJoNgp7FSEF4pc9K5l7TiExcQn-GtA_OFaP4bpNCUvq42EXUsapsPDdIhLGIAKE6LFz9Lvj8jmu4v8baAvC4iCdrBz_TQ0Clt6CG2OglQY37qXnEfM2Sq1nP0H0KkIjM-ZFUQRvAjGiXDCpIWVna4MCuNb9xVBeuiR8sSS7mRG3u8DHFkmqoEemXGbeKtTjgCLzHENyRDq0wMI34UwFDPUDIdUHT3lXecd-ssdoY6VtUxXtWf8iukaRBV2GbAccUjYyY28JLGvB4lw3r8DSVB7n_7t2IElUAd47OoOH9NSvWMRCOeG8bJTS_IO8In3kb4WbArFEgMY7xK2xw1_v2w/file [following]\n",
            "--2021-02-21 07:00:27--  https://uc57a9415f9781202ce5776f8af5.dl.dropboxusercontent.com/cd/0/inline2/BJW6U4UtgeUaXS9qv-sau6hDPpJoNgp7FSEF4pc9K5l7TiExcQn-GtA_OFaP4bpNCUvq42EXUsapsPDdIhLGIAKE6LFz9Lvj8jmu4v8baAvC4iCdrBz_TQ0Clt6CG2OglQY37qXnEfM2Sq1nP0H0KkIjM-ZFUQRvAjGiXDCpIWVna4MCuNb9xVBeuiR8sSS7mRG3u8DHFkmqoEemXGbeKtTjgCLzHENyRDq0wMI34UwFDPUDIdUHT3lXecd-ssdoY6VtUxXtWf8iukaRBV2GbAccUjYyY28JLGvB4lw3r8DSVB7n_7t2IElUAd47OoOH9NSvWMRCOeG8bJTS_IO8In3kb4WbArFEgMY7xK2xw1_v2w/file\n",
            "Reusing existing connection to uc57a9415f9781202ce5776f8af5.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13122229 (13M) [application/zip]\n",
            "Saving to: ‘BEST-TrainingSet.zip.3’\n",
            "\n",
            "BEST-TrainingSet.zi 100%[===================>]  12.51M  11.6MB/s    in 1.1s    \n",
            "\n",
            "2021-02-21 07:00:29 (11.6 MB/s) - ‘BEST-TrainingSet.zip.3’ saved [13122229/13122229]\n",
            "\n",
            "--2021-02-21 07:00:29--  https://www.dropbox.com/s/n87fiy25f2yc3gt/wiki.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/n87fiy25f2yc3gt/wiki.zip [following]\n",
            "--2021-02-21 07:00:29--  https://www.dropbox.com/s/raw/n87fiy25f2yc3gt/wiki.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1fec1d5b731e251982cf03e6c8.dl.dropboxusercontent.com/cd/0/inline/BJWrHV5byjR3xd8nEbdBCJnzjZ-Lh4-kyp3t5Znj1Rjsl49Xc6ifbvteIeJCmc0DqftQAiA_67eiy-FJV_mHf-LRSHrsIfSqi7UTViCQgiu9EJypLdArVvjZskx0L6qq9FU/file# [following]\n",
            "--2021-02-21 07:00:30--  https://uc1fec1d5b731e251982cf03e6c8.dl.dropboxusercontent.com/cd/0/inline/BJWrHV5byjR3xd8nEbdBCJnzjZ-Lh4-kyp3t5Znj1Rjsl49Xc6ifbvteIeJCmc0DqftQAiA_67eiy-FJV_mHf-LRSHrsIfSqi7UTViCQgiu9EJypLdArVvjZskx0L6qq9FU/file\n",
            "Resolving uc1fec1d5b731e251982cf03e6c8.dl.dropboxusercontent.com (uc1fec1d5b731e251982cf03e6c8.dl.dropboxusercontent.com)... 162.125.64.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to uc1fec1d5b731e251982cf03e6c8.dl.dropboxusercontent.com (uc1fec1d5b731e251982cf03e6c8.dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BJXfyyFkWWo_ZZa9OhlBES3KsfvKus13pBlktdaqJewMcrGffcFkhtZGb54YzMVVyebgToC88U1MtXfJUf9g2Kp8Ky58Fu4OCc2zH8yv4W6eTDZkvZsqaxbAuzq9lbgz155C7wydu4OxEF8KBD6FIcjxd9n9R6_vIlWCXqz8-U-hOEFPjKWW3Hn35ePvaTpH-_CyBfNtGVFXnFQTV6prc_Isny39ObG9zKefxSCPlx0kbH5F60FxQisYNjtXC7UmldiCVsMzy6P8j_jRJN9ML30J6VmyX015sEYyzBFIGB9WJr2QVlkldmGwXCbN8zAlrE6YLmcwEVugduzvbzkH9iPVcF-V6RMM7DCZa-1kqQxW1g/file [following]\n",
            "--2021-02-21 07:00:30--  https://uc1fec1d5b731e251982cf03e6c8.dl.dropboxusercontent.com/cd/0/inline2/BJXfyyFkWWo_ZZa9OhlBES3KsfvKus13pBlktdaqJewMcrGffcFkhtZGb54YzMVVyebgToC88U1MtXfJUf9g2Kp8Ky58Fu4OCc2zH8yv4W6eTDZkvZsqaxbAuzq9lbgz155C7wydu4OxEF8KBD6FIcjxd9n9R6_vIlWCXqz8-U-hOEFPjKWW3Hn35ePvaTpH-_CyBfNtGVFXnFQTV6prc_Isny39ObG9zKefxSCPlx0kbH5F60FxQisYNjtXC7UmldiCVsMzy6P8j_jRJN9ML30J6VmyX015sEYyzBFIGB9WJr2QVlkldmGwXCbN8zAlrE6YLmcwEVugduzvbzkH9iPVcF-V6RMM7DCZa-1kqQxW1g/file\n",
            "Reusing existing connection to uc1fec1d5b731e251982cf03e6c8.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92415438 (88M) [application/zip]\n",
            "Saving to: ‘wiki.zip.3’\n",
            "\n",
            "wiki.zip.3          100%[===================>]  88.13M  22.5MB/s    in 4.3s    \n",
            "\n",
            "2021-02-21 07:00:35 (20.5 MB/s) - ‘wiki.zip.3’ saved [92415438/92415438]\n",
            "\n",
            "Archive:  wiki.zip\n",
            "replace __MACOSX/._wiki? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  BEST-TrainingSet.zip\n",
            "replace __MACOSX/._BEST-TrainingSet? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ALwvtzDZ-f"
      },
      "source": [
        "#Step 1: read the wikipedia text file\n",
        "with open(\"wiki/thwiki_chk.txt\") as f:\n",
        "    #the delimiter is one or more whitespace characters\n",
        "    input_text = re.compile(r\"\\s+\").split(f.read()) \n",
        "    #exclude an empty string from our input\n",
        "    input_text = [word for word in input_text if word != ''] "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXoFAfjaDcJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f9c3968e-4d01-46ce-8331-370f88f9e30b"
      },
      "source": [
        "tokens = input_text\n",
        "print(tokens[:10])\n",
        "print(\"total word count:\", len(tokens))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['หน้า', 'หลัก', 'วิกิพีเดีย', 'ดำเนินการ', 'โดย', 'มูลนิธิ', 'วิกิ', 'มีเดีย', 'องค์กร', 'ไม่']\n",
            "total word count: 36349066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDVMvTRci-Xu"
      },
      "source": [
        "# Step 2: Indexing (Assign a number to each word)\n",
        "\n",
        "The code below generates an indexed dataset(each word is represented by a number), a dictionary, a reversed dictionary\n",
        "\n",
        "## <font color=''>Homework Question 1:</font>\n",
        "<font color=''>“UNK” is often used to represent an unknown word (a word which does not exist in your dictionary/training set). You can also represent a rare word with this token as well.  How do you define a rare word in your program? Explain in your own words and capture the screenshot of your code segment that is a part of this process</font>\n",
        "\n",
        " + <font color=''>edit or replace create_index with your own code to set a threshold for rare words and replace them with \"UNK\"</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6NP7nQGi-Xw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8feae796-5d08-45ea-d51d-600849bde55f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "word_count_global = None\n",
        "#step 2:Build dictionary and build a dataset(replace each word with its index)\n",
        "def create_index(input_text, min_thres_unk = 0, max_word_count = None):\n",
        "    global word_count_global\n",
        "    # TODO#1 : edit or replace this function\n",
        "    words = [word for word in input_text ]\n",
        "    word_count = list()\n",
        "\n",
        "    #use set and len to get the number of unique words\n",
        "    word_count.extend(collections.Counter(words).most_common(len(set(words))))\n",
        "    #include a token for unknown word\n",
        "    word_count.append((\"UNK\",0))\n",
        "    word_count_global = word_count\n",
        "\n",
        "    #print out 10 most frequent words\n",
        "    print(word_count[:10])\n",
        "\n",
        "    dictionary = dict()\n",
        "    dictionary[\"for_keras_zero_padding\"] = 0\n",
        "    for word, count in word_count:\n",
        "      if count > min_thres_unk or word == 'UNK':\n",
        "        dictionary[word] = len(dictionary)\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    data = list()\n",
        "    for word in input_text:\n",
        "      if word not in dictionary:\n",
        "        data.append(dictionary['UNK'])\n",
        "      else:\n",
        "        data.append(dictionary[word])\n",
        "\n",
        "    return data,dictionary, reverse_dictionary\n",
        "\n",
        "# call method with min_thres_unk=1ß\n",
        "dataset, dictionary, reverse_dictionary = create_index(tokens, 3)\n",
        "print(len(dataset))\n",
        "print(len(dictionary))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ที่', 950006), ('ใน', 897329), ('เป็น', 726847), ('และ', 668116), ('การ', 619128), ('มี', 536738), ('ของ', 532237), ('ได้', 508117), (')', 359576), ('\"', 357830)]\n",
            "36349066\n",
            "153544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1AHl_hclFEP"
      },
      "source": [
        "## Explain\r\n",
        "I plot frequency of words to see what threshold shall I cut<br>\r\n",
        "Here, I chose `threshold 3` 78.0% (of words)<br>\r\n",
        "because after this number of words increase by only 1 percent which, to me, is not significant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "-Qnsaognfwql",
        "outputId": "62542feb-1345-4be1-9097-2e3971a6bf6f"
      },
      "source": [
        "bins = plt.hist([c for w,c in word_count_global], range(0,20))\r\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qUaRjN5YhmPU",
        "outputId": "5318e7ef-8b83-490e-ffc7-5b5aa1de1555"
      },
      "source": [
        "N = 701356\r\n",
        "for i in range(1,20):\r\n",
        "  n = sum(bins[0][1:i])\r\n",
        "  print('threshold', i-1, n, f'{10000 * n / N // 100}%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "threshold 0 0 0.0%\n",
            "threshold 1 406192.0 57.0%\n",
            "threshold 2 503531.0 71.0%\n",
            "threshold 3 547812.0 78.0%\n",
            "threshold 4 574133.0 81.0%\n",
            "threshold 5 591453.0 84.0%\n",
            "threshold 6 604487.0 86.0%\n",
            "threshold 7 614252.0 87.0%\n",
            "threshold 8 621916.0 88.0%\n",
            "threshold 9 628181.0 89.0%\n",
            "threshold 10 633380.0 90.0%\n",
            "threshold 11 637842.0 90.0%\n",
            "threshold 12 641602.0 91.0%\n",
            "threshold 13 644887.0 91.0%\n",
            "threshold 14 647841.0 92.0%\n",
            "threshold 15 650431.0 92.0%\n",
            "threshold 16 652804.0 93.0%\n",
            "threshold 17 654832.0 93.0%\n",
            "threshold 18 658550.0 93.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fotaYMgi-Xz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "de6f4160-83e4-4b5c-f2ad-a0fb8e622607"
      },
      "source": [
        "print(\"output sample (dataset):\",dataset[:10])\n",
        "print(\"output sample (dictionary):\",{k: dictionary[k] for k in list(dictionary)[:10]})\n",
        "print(\"output sample (reverse dictionary):\",{k: reverse_dictionary[k] for k in list(reverse_dictionary)[:10]})"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output sample (dataset): [229, 208, 2453, 573, 15, 1829, 7149, 3124, 681, 24]\n",
            "output sample (dictionary): {'for_keras_zero_padding': 0, 'ที่': 1, 'ใน': 2, 'เป็น': 3, 'และ': 4, 'การ': 5, 'มี': 6, 'ของ': 7, 'ได้': 8, ')': 9}\n",
            "output sample (reverse dictionary): {0: 'for_keras_zero_padding', 1: 'ที่', 2: 'ใน', 3: 'เป็น', 4: 'และ', 5: 'การ', 6: 'มี', 7: 'ของ', 8: 'ได้', 9: ')'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HutTzPO7i-X3"
      },
      "source": [
        "# Step3: Create skip-grams (inputs for your model)\n",
        "Keras has a skipgrams-generator, the cell below shows us how it generates skipgrams \n",
        "\n",
        "## <font color=''>Homework Question 2:</font>\n",
        "<font color=''>The negative samples are sampled from sampling_table.  Look through Keras source code to find out how they sample negative samples. Discuss the sampling technique taught in class and compare it to the Keras source code.</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwYFRO3YGryQ"
      },
      "source": [
        "<font color=''>Q2: PUT YOUR ANSER HERE!!!</font><br>\r\n",
        "Keras tries to decrease probabilty of words of high frequency, while in the class, we try to increase that of words of low frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C520WnI0i-X4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1c2f0405-ef3a-4c51-fb3a-ffd2f9c0032a"
      },
      "source": [
        "# Step 3: Create data samples\n",
        "vocab_size = len(dictionary)\n",
        "skip_window = 1       # How many words to consider left and right.\n",
        "\n",
        "# TODO#2 check out keras source code and find out how their sampling technique works. Describe it in your own words.\n",
        "sample_set= dataset[:10]\n",
        "sampling_table = sequence.make_sampling_table(vocab_size)\n",
        "couples, labels = skipgrams(sample_set, vocab_size, window_size=skip_window, sampling_table=sampling_table)\n",
        "word_target, word_context = zip(*couples)\n",
        "word_target = np.array(word_target, dtype=\"int32\")\n",
        "word_context = np.array(word_context, dtype=\"int32\")\n",
        "\n",
        "print(couples, labels)\n",
        "\n",
        "for i in range(8):\n",
        "  print(reverse_dictionary[couples[i][0]],reverse_dictionary[couples[i][1]])\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[208, 41854], [208, 2453], [2453, 573], [208, 117758], [24, 681], [2453, 208], [3124, 72927], [2453, 57788], [24, 1704], [208, 229], [3124, 681], [3124, 7149], [3124, 142854], [2453, 109975]] [0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "หลัก ซัมเมอร์ส\n",
            "หลัก วิกิพีเดีย\n",
            "วิกิพีเดีย ดำเนินการ\n",
            "หลัก คอนเวอร์เตอร์\n",
            "ไม่ องค์กร\n",
            "วิกิพีเดีย หลัก\n",
            "มีเดีย เหรียญศาร\n",
            "วิกิพีเดีย แวร์ซาย\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6UL0FhEi-X6"
      },
      "source": [
        "# Step 4: create the skip-gram model\n",
        "## <font color=''>Homework Question 3:</font>\n",
        " <font color=''>Q3:  In your own words, discuss why Sigmoid is chosen as the activation function in the  skip-gram model.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oQLGkkuHG7o"
      },
      "source": [
        "<font color='red'>Q3: PUT YOUR ANSER HERE!!!</font>\r\n",
        " <br>\r\n",
        " First of all, it is memory friendly. No matter how we calculate cosine similarity of a pair of words. The output is only one number instead of N-dim vector.<br>\r\n",
        " In addition, loss function uses log which make much more sense with [0,1] for probability rather than relu [0,inf) or tanh [-1,1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq7Eh9pXi-X7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cc42a939-dbde-40c2-9f0a-f707a6740d8f"
      },
      "source": [
        "#reference: https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb\n",
        "dim_embedddings = 32\n",
        "V= len(dictionary)\n",
        "\n",
        "#step1: select the embedding of the target word from W\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "w = Embedding(V+1, dim_embedddings)(w_inputs)\n",
        "\n",
        "#step2: select the embedding of the context word from C\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "c  = Embedding(V+1, dim_embedddings)(c_inputs)\n",
        "\n",
        "#step3: compute the dot product:c_k*v_j\n",
        "o = Dot(axes=2)([w, c])\n",
        "o = Reshape((1,), input_shape=(1, 1))(o)\n",
        "\n",
        "#step4: normailize dot products into probability\n",
        "o = Activation('sigmoid')(o)\n",
        "#TO DO#4 Question: Why sigmoid?\n",
        "\n",
        "SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n",
        "SkipGram.summary()\n",
        "opt=Adam(lr=0.01)\n",
        "SkipGram.compile(loss='binary_crossentropy', optimizer=opt)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 32)        4913440     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1, 32)        4913440     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
            "                                                                 embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 1)            0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 1)            0           reshape[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 9,826,880\n",
            "Trainable params: 9,826,880\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgR5p_h1i-X9"
      },
      "source": [
        "# you don't have to spend too much time training for your homework, you are allowed to do it on a smaller corpus\n",
        "# currently the dataset is 1/20 of the full text file.\n",
        "multiplier = 100000\n",
        "for _ in range(1):\n",
        "    prev_i=0\n",
        "    #it is likely that your GPU won't be able to handle large input\n",
        "    #just do it 100000 words at a time\n",
        "    for i in range(236, len(dataset)//multiplier):\n",
        "        #generate skipgrams\n",
        "        data, labels = skipgrams(sequence=dataset[prev_i*multiplier:(i*multiplier)+multiplier], vocabulary_size=V, window_size=2, negative_samples=4.)\n",
        "        x = [np.array(x) for x in zip(*data)]\n",
        "        y = np.array(labels, dtype=np.int32)\n",
        "        if x:\n",
        "            loss = SkipGram.train_on_batch(x, y)\n",
        "        prev_i = i \n",
        "        print(loss,i*multiplier)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY69_WFHi-X_"
      },
      "source": [
        "# SkipGram.save_weights('my_skipgram32_weights-hw.h5')\r\n",
        "SkipGram.load_weights('my_skipgram32_weights-hw.h5')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7UD13eKki-YA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3e3060ee-70a2-4f99-b571-9822d7f3f5b7"
      },
      "source": [
        "#Get weight of the embedding layer\n",
        "final_embeddings=SkipGram.get_weights()[0]\n",
        "print(final_embeddings)\n",
        "print(final_embeddings.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.01860917  0.01938124  0.0207364  ...  0.01218445  0.00212651\n",
            "   0.01375028]\n",
            " [-0.49981844  0.41046733 -0.59824306 ...  0.5241086  -0.5860986\n",
            "   0.5765467 ]\n",
            " [-0.50383323  0.28770196 -0.45041224 ...  0.4926264  -0.5751475\n",
            "   0.6041441 ]\n",
            " ...\n",
            " [ 0.02654959  0.02232834 -0.04700992 ...  0.01880474  0.02930037\n",
            "  -0.02829891]\n",
            " [ 0.03735652  0.00350807 -0.01710014 ...  0.04450684  0.00997959\n",
            "   0.03114747]\n",
            " [-0.46685266  0.44005847 -0.47300085 ...  0.4222149  -0.5279841\n",
            "   0.52296746]]\n",
            "(153545, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ovPmh6Ri-YC"
      },
      "source": [
        "# Step 5: Intrinsic Evaluation: Word Vector Analogies\n",
        "## <font color='blue'>Homework Question 4: </font>\n",
        "<font color='blue'> Read section 2.1 and 2.3 in this [lecture note](http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf). Come up with 10 semantic analogy examples and report results produced by your word embeddings </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8rTxYaLi-YD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f872dea-9acf-46a3-ce34-8e6b5d191150"
      },
      "source": [
        "# TODO#4:Come up with 10 semantic analogy examples and report results produced by your word embeddings \n",
        "#and tell us what you observe \n",
        "\n",
        "import scipy.spatial\n",
        "s = [\n",
        "     ('แมว','หมา','สิงโต'),\n",
        "     ('เหลือง','แดง','เขียว'),\n",
        "     ('โตเกียว','ญี่ปุ่น','ไทย'),\n",
        "     ('king','man', 'woman'),\n",
        "     ('ขอทาน','จน','รวย'),\n",
        "     ('เพลง','ศิลปิน','ทหาร'),\n",
        "     ('งง','โง่','ฉลาด'),\n",
        "     ('ล้น','มาก','น้อย'),\n",
        "     ('ชอบ','รัก','หลง'),\n",
        "     ('กษัตริย์','ชาย','หญิง')\n",
        "    ]\n",
        "for i in s:\n",
        "  tmp = (final_embeddings[dictionary[i[0]]]-final_embeddings[dictionary[i[1]]]+final_embeddings[dictionary[i[2]]]).reshape(1,-1)\n",
        "  all_dist = scipy.spatial.distance.cdist(final_embeddings, tmp, 'cosine').reshape(-1)\n",
        "  idx = np.argpartition(all_dist,1)[:1][0]\n",
        "  # print(reverse_dictionary[idx])\n",
        "  # idx = idx[np.argsort(all_dist[idx[:3]])]\n",
        "  # for i in idx:\n",
        "  print(i[0],'-',i[1],'+',i[2],'=',reverse_dictionary[idx])\n",
        "  print('===========')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "แมว - หมา + สิงโต = สิงโต\n",
            "===========\n",
            "เหลือง - แดง + เขียว = เหลือง\n",
            "===========\n",
            "โตเกียว - ญี่ปุ่น + ไทย = อคติ\n",
            "===========\n",
            "king - man + woman = ฮนโนจิ\n",
            "===========\n",
            "ขอทาน - จน + รวย = who\n",
            "===========\n",
            "เพลง - ศิลปิน + ทหาร = เถรวาท\n",
            "===========\n",
            "งง - โง่ + ฉลาด = ญาณ\n",
            "===========\n",
            "ล้น - มาก + น้อย = จอร์จ\n",
            "===========\n",
            "ชอบ - รัก + หลง = หลง\n",
            "===========\n",
            "กษัตริย์ - ชาย + หญิง = เอสปา\n",
            "===========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnBg4z4xSCYB"
      },
      "source": [
        "The embedding can not catch the meaning of the word that much, yet <br>\r\n",
        "`กษัตริย์ - ชาย + หญิง = เอสปา` is kinda cool, becase `เอสปา` is girl group and might be \"queen\" for their fans <br>\r\n",
        "`งง - โง่ + ฉลาด = ญาณ` is also cool, since `ญาณ` is `ปัญญา` in Buddhism<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLqG8WaNi-YE"
      },
      "source": [
        "# Step 6: Extrinsic Evaluation\n",
        "\n",
        "## <font color='blue'>Homework Question5:</font>\n",
        "<font color='blue'>\n",
        "Use the word embeddings from the skip-gram model as pre-trained weights in a classification model. Compare the result the with the same classification model that does not use the pre-trained weights. \n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBPutcxEi-YF"
      },
      "source": [
        "all_news_filepath = glob.glob('BEST-TrainingSet/news/*.txt')\n",
        "all_novel_filepath = glob.glob('BEST-TrainingSet/novel/*.txt')\n",
        "all_article_filepath = glob.glob('BEST-TrainingSet/article/*.txt')\n",
        "all_encyclopedia_filepath = glob.glob('BEST-TrainingSet/encyclopedia/*.txt')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaX-L5n4i-YG"
      },
      "source": [
        "#preparing data for the classificaiton model\n",
        "#In your homework, we will only use the first 2000 words in each text file\n",
        "#any text file that has less than 2000 words will be padded\n",
        "#reason:just to make this homework feasible under limited time and resource\n",
        "max_length = 2000\n",
        "def word_to_index(word):\n",
        "    if word in dictionary:\n",
        "        return dictionary[word]\n",
        "    else:#if unknown\n",
        "        return dictionary[\"UNK\"]\n",
        "\n",
        "\n",
        "def prep_data():\n",
        "    input_text = list()\n",
        "    for textfile_path in [all_news_filepath, all_novel_filepath, all_article_filepath, all_encyclopedia_filepath]:\n",
        "        for input_file in textfile_path:\n",
        "            f = open(input_file,\"r\") #open file with name of \"*.txt\"\n",
        "            text = re.sub(r'\\|', ' ', f.read()) # replace separation symbol with white space           \n",
        "            text = re.sub(r'<\\W?\\w+>', '', text)# remove <NE> </NE> <AB> </AB> tags\n",
        "            text = text.split() #split() method without an argument splits on whitespace \n",
        "            indexed_text = list(map(lambda x:word_to_index(x), text[:max_length])) #map raw word string to its index   \n",
        "            if 'news' in input_file:\n",
        "                input_text.append([indexed_text,0]) \n",
        "            elif 'novel' in input_file:\n",
        "                input_text.append([indexed_text,1]) \n",
        "            elif 'article' in input_file:\n",
        "                input_text.append([indexed_text,2]) \n",
        "            elif 'encyclopedia' in input_file:\n",
        "                input_text.append([indexed_text,3]) \n",
        "            \n",
        "            f.close()\n",
        "    random.shuffle(input_text)\n",
        "    return input_text\n",
        "\n",
        "input_data = prep_data()\n",
        "train_data = input_data[:int(len(input_data)*0.6)]\n",
        "val_data = input_data[int(len(input_data)*0.6):int(len(input_data)*0.8)]\n",
        "test_data = input_data[int(len(input_data)*0.8):]\n",
        "\n",
        "train_input = [data[0] for data in train_data]\n",
        "train_input = sequence.pad_sequences(train_input, maxlen=max_length) #padding\n",
        "train_target = [data[1] for data in train_data]\n",
        "train_target=to_categorical(train_target, num_classes=4)\n",
        "\n",
        "val_input = [data[0] for data in val_data]\n",
        "val_input = sequence.pad_sequences(val_input, maxlen=max_length) #padding\n",
        "val_target = [data[1] for data in val_data]\n",
        "val_target=to_categorical(val_target, num_classes=4)\n",
        "\n",
        "test_input = [data[0] for data in test_data]\n",
        "test_input = sequence.pad_sequences(test_input, maxlen=max_length) #padding\n",
        "test_target = [data[1] for data in test_data]\n",
        "test_target=to_categorical(test_target, num_classes=4)\n",
        "\n",
        "del input_data, val_data,train_data, test_data"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjsAjAyOcr9T",
        "outputId": "a3c9aa6b-7fe6-4137-948b-c154bfae5e4d"
      },
      "source": [
        "val_input"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[153544,  25982,    207, ..., 153544, 153544,   1089],\n",
              "       [  3382,     39,      1, ...,    202,    130,     13],\n",
              "       [  8544,    156,     41, ...,   5572,     58,    759],\n",
              "       ...,\n",
              "       [  4313,    907,   1273, ...,  23587,    444,     87],\n",
              "       [    74,     55,   5659, ...,    510,    266,    203],\n",
              "       [    27,     17,    750, ...,      1,     50,      8]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syrKnUxWi-YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc429071-a118-4b02-a68d-774c087fa430"
      },
      "source": [
        "#the classification model\n",
        "#TODO#5 find out how to initialize your embedding layer with pre-trained weights, evaluate and observe\n",
        "#don't forget to compare it with the same model that does not use pre-trained weights\n",
        "#you can use your own model too! and feel free to customize this model as you wish\n",
        "cls_model = Sequential()\n",
        "cls_model.add(Embedding(len(dictionary)+1, 32, input_length=max_length,mask_zero=True))\n",
        "cls_model.add(GRU(32))\n",
        "cls_model.add(Dropout(0.5))\n",
        "cls_model.add(Dense(4, activation='softmax'))\n",
        "opt=Adam(lr=0.01)\n",
        "cls_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "cls_model.summary()\n",
        "print('Train...')\n",
        "cls_model.fit(train_input, train_target,\n",
        "          epochs=10,\n",
        "          validation_data=[val_input, val_target])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 2000, 32)          4913440   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 32)                6336      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 4,919,908\n",
            "Trainable params: 4,919,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 33s 3s/step - loss: 1.3683 - accuracy: 0.3625 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 27s 3s/step - loss: 1.2260 - accuracy: 0.4468 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.6871 - accuracy: 0.7648 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.3281 - accuracy: 0.9006 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 27s 3s/step - loss: 0.1906 - accuracy: 0.9638 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.0472 - accuracy: 0.9942 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7f741234a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t_dK8l9H92h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9934067-e7a3-4c94-bcc4-7144895712a4"
      },
      "source": [
        "results = cls_model.evaluate(test_input, test_target)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 3s 103ms/step - loss: 2.5718 - accuracy: 0.4902\n",
            "test loss, test acc: [2.571761131286621, 0.4901960790157318]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wZU9eEkHy2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877a701e-ce9e-4c2a-fc3b-ea389505a67f"
      },
      "source": [
        "cls_model_with_pretrained_weight = Sequential()\r\n",
        "cls_model_with_pretrained_weight.add(Embedding(len(dictionary)+1, 32, input_length=max_length,mask_zero=True, embeddings_initializer=tf.keras.initializers.Constant(final_embeddings)))\r\n",
        "cls_model_with_pretrained_weight.add(GRU(32))\r\n",
        "cls_model_with_pretrained_weight.add(Dropout(0.5))\r\n",
        "cls_model_with_pretrained_weight.add(Dense(4, activation='softmax'))\r\n",
        "opt=Adam(lr=0.01)\r\n",
        "cls_model_with_pretrained_weight.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\r\n",
        "cls_model_with_pretrained_weight.summary()\r\n",
        "print('Train...')\r\n",
        "cls_model_with_pretrained_weight.fit(train_input, train_target,\r\n",
        "          epochs=10,\r\n",
        "          validation_data=[val_input, val_target])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 2000, 32)          4913440   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 32)                6336      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 4,919,908\n",
            "Trainable params: 4,919,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 22s 2s/step - loss: 1.4773 - accuracy: 0.2811 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 1.4105 - accuracy: 0.3412 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 24s 3s/step - loss: 1.3339 - accuracy: 0.4119 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 27s 3s/step - loss: 1.2522 - accuracy: 0.4650 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 28s 3s/step - loss: 1.0918 - accuracy: 0.6006 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.8065 - accuracy: 0.7821 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 24s 2s/step - loss: 0.4378 - accuracy: 0.8820 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 17s 2s/step - loss: 0.2755 - accuracy: 0.9177 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.1611 - accuracy: 0.9722 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 30s 3s/step - loss: 0.1203 - accuracy: 0.9713 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7f6e2591d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEnvQKECTsqk",
        "outputId": "9283f43d-f0ea-4a69-d559-8535d3585e39"
      },
      "source": [
        "results = cls_model_with_pretrained_weight.evaluate(test_input, test_target)\r\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 2s 105ms/step - loss: 1.7496 - accuracy: 0.4706\n",
            "test loss, test acc: [1.749598503112793, 0.47058823704719543]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ086voBWJWk"
      },
      "source": [
        "### Discussion\r\n",
        "With pretrained weight, model converge better.<br>\r\n",
        "However the same analogy we used seem to be confusing now...\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txaPcJ1iTwOA"
      },
      "source": [
        "model_final_embeddings = cls_model.layers[0]\r\n",
        "model_final_embeddings_with_pretrained_weight = cls_model_with_pretrained_weight.layers[0]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROSLv_jtWafG",
        "outputId": "7bd54e3c-46e0-42eb-a73c-248b34274c05"
      },
      "source": [
        "print('Without pretrain')\r\n",
        "result_embedding = model_final_embeddings.weights[0].numpy()\r\n",
        "for i in s:\r\n",
        "  tmp = (result_embedding[dictionary[i[0]]]-result_embedding[dictionary[i[1]]]+result_embedding[dictionary[i[2]]]).reshape(1,-1)\r\n",
        "  all_dist = scipy.spatial.distance.cdist(result_embedding, tmp, 'cosine').reshape(-1)\r\n",
        "  idx = np.argpartition(all_dist,1)[:1][0]\r\n",
        "  # print(reverse_dictionary[idx])\r\n",
        "  # idx = idx[np.argsort(all_dist[idx[:3]])]\r\n",
        "  # for i in idx:\r\n",
        "  print(i[0],'-',i[1],'+',i[2],'=',reverse_dictionary[idx])\r\n",
        "  print('===========')\r\n",
        "print('\\n\\n')\r\n",
        "print('With pretrain')\r\n",
        "result_embedding = model_final_embeddings_with_pretrained_weight.weights[0].numpy()\r\n",
        "for i in s:\r\n",
        "  tmp = (result_embedding[dictionary[i[0]]]-result_embedding[dictionary[i[1]]]+result_embedding[dictionary[i[2]]]).reshape(1,-1)\r\n",
        "  all_dist = scipy.spatial.distance.cdist(result_embedding, tmp, 'cosine').reshape(-1)\r\n",
        "  idx = np.argpartition(all_dist,1)[:1][0]\r\n",
        "  # print(reverse_dictionary[idx])\r\n",
        "  # idx = idx[np.argsort(all_dist[idx[:3]])]\r\n",
        "  # for i in idx:\r\n",
        "  print(i[0],'-',i[1],'+',i[2],'=',reverse_dictionary[idx])\r\n",
        "  print('===========')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without pretrain\n",
            "แมว - หมา + สิงโต = เช่น\n",
            "===========\n",
            "เหลือง - แดง + เขียว = สัมภาษณ์\n",
            "===========\n",
            "โตเกียว - ญี่ปุ่น + ไทย = พระบาท\n",
            "===========\n",
            "king - man + woman = king\n",
            "===========\n",
            "ขอทาน - จน + รวย = รั้ว\n",
            "===========\n",
            "เพลง - ศิลปิน + ทหาร = )\n",
            "===========\n",
            "งง - โง่ + ฉลาด = งง\n",
            "===========\n",
            "ล้น - มาก + น้อย = Peace\n",
            "===========\n",
            "ชอบ - รัก + หลง = 58\n",
            "===========\n",
            "กษัตริย์ - ชาย + หญิง = โฆษณา\n",
            "===========\n",
            "\n",
            "\n",
            "\n",
            "With pretrain\n",
            "แมว - หมา + สิงโต = ชิน\n",
            "===========\n",
            "เหลือง - แดง + เขียว = โกลิยะ\n",
            "===========\n",
            "โตเกียว - ญี่ปุ่น + ไทย = ไทย\n",
            "===========\n",
            "king - man + woman = ฮนโนจิ\n",
            "===========\n",
            "ขอทาน - จน + รวย = ๑๐๑\n",
            "===========\n",
            "เพลง - ศิลปิน + ทหาร = ๕\n",
            "===========\n",
            "งง - โง่ + ฉลาด = ญาณ\n",
            "===========\n",
            "ล้น - มาก + น้อย = น้อย\n",
            "===========\n",
            "ชอบ - รัก + หลง = ยืนยัน\n",
            "===========\n",
            "กษัตริย์ - ชาย + หญิง = สมการ\n",
            "===========\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0wJOAg2eoQk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}